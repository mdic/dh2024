{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f17bf31-e207-4bc7-aaee-fe9ae7fe268e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This script is adapted from Corpus Approaches to Language in Social Media Online Companion (Di Cristofaro 2023), https://catlism.github.io and licensed under GPLv3.\n",
    "# Import modules for: working on local folders and files; regular expressions; finding files in a folder; reading JSON files;\n",
    "# using BeautifulSoup; working with XML files\n",
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42b74301-a243-45fb-8e22-21ac7148799b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 78), match='The ethical dilemma of self-driving cars - Patric>\n",
      "<re.Match object; span=(0, 86), match='The Turing test： Can a computer pass for a human？>\n",
      "<re.Match object; span=(0, 62), match='Can robots be creative？ - Gil Weinberg [Rh9vBczqM>\n",
      "<re.Match object; span=(0, 67), match='The most important century in human history [-T__>\n",
      "<re.Match object; span=(0, 88), match='The Turing test： Can a computer pass for a human？>\n",
      "<re.Match object; span=(0, 65), match='The most important century in human history [-T__>\n",
      "<re.Match object; span=(0, 73), match='Can machines read your emotions？ - Kostas Karpouz>\n",
      "<re.Match object; span=(0, 53), match='How will AI change the world？ [RzkD_rTEBYs].info.>\n",
      "<re.Match object; span=(0, 75), match='Can machines read your emotions？ - Kostas Karpouz>\n",
      "<re.Match object; span=(0, 51), match='How will AI change the world？ [RzkD_rTEBYs].en.sr>\n",
      "<re.Match object; span=(0, 79), match='How does artificial intelligence learn？ - Briana >\n",
      "<re.Match object; span=(0, 81), match='How does artificial intelligence learn？ - Briana >\n",
      "<re.Match object; span=(0, 76), match='The ethical dilemma of self-driving cars - Patric>\n",
      "<re.Match object; span=(0, 60), match='Can robots be creative？ - Gil Weinberg [Rh9vBczqM>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create a regular expression to capture the title of the video preceding yt-dlphttps://www.youtube.com/watch?v=8LHSY0zmrrU default naming conventions, where:\n",
    "# [FILENAME].info.json = the JSON file containing the metadata details\n",
    "# [FILENAME].[LL].srv3 = the XML file containing the subtitles in SRV3 format, where [LL] is the 2-letter ISO 3166-1 language code\n",
    "# The regex reads \n",
    "filename_filter = re.compile(r\"(.*?)\\.(info.json|[A-Za-z]{1,3}\\.srv3)\")\n",
    "# Create an empty list to store all the video titles\n",
    "unique_filenames_list = []\n",
    "# List all filenames present in the folder where the script resides\n",
    "files = glob(\"*.*\")\n",
    "\n",
    "# For every single filename found in the folder, do:\n",
    "for single_file in files:\n",
    "    # Search for the regular expression for capturing metadata and subtitle files in the filename, and store the result\n",
    "    # in the 'found_filename' variable\n",
    "    found_filename = re.search(filename_filter, single_file)\n",
    "    print(found_filename)\n",
    "    # If the filename matches the regular expression, extract the filename without the extensions; then check if the cleaned\n",
    "    # filename is present in the unique_filenames_list, and if not add it\n",
    "    if found_filename is not None and found_filename[1] not in unique_filenames_list:\n",
    "        unique_filenames_list.append(found_filename[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79494420-e0b5-47b9-8403-e65d64aeec56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing The ethical dilemma of self-driving cars - Patrick Lin [ixIoDYVfKA0].info.json\n",
      "Processing The ethical dilemma of self-driving cars - Patrick Lin [ixIoDYVfKA0].en.srv3\n",
      "Processing The Turing test： Can a computer pass for a human？ - Alex Gendler [3wLqsRLvV-c].info.json\n",
      "Processing The Turing test： Can a computer pass for a human？ - Alex Gendler [3wLqsRLvV-c].en.srv3\n",
      "Processing Can robots be creative？ - Gil Weinberg [Rh9vBczqMk0].info.json\n",
      "Processing Can robots be creative？ - Gil Weinberg [Rh9vBczqMk0].en.srv3\n",
      "Processing The most important century in human history [-T__YWoq45I].info.json\n",
      "Processing The most important century in human history [-T__YWoq45I].en.srv3\n",
      "Processing Can machines read your emotions？ - Kostas Karpouzis [QFk3e5PcK7s].info.json\n",
      "Processing Can machines read your emotions？ - Kostas Karpouzis [QFk3e5PcK7s].en.srv3\n",
      "Processing How will AI change the world？ [RzkD_rTEBYs].info.json\n",
      "Processing How will AI change the world？ [RzkD_rTEBYs].en.srv3\n",
      "Processing How does artificial intelligence learn？ - Briana Brownell [0yCJMt9Mx9c].info.json\n",
      "Processing How does artificial intelligence learn？ - Briana Brownell [0yCJMt9Mx9c].en.srv3\n"
     ]
    }
   ],
   "source": [
    "# For each unique filename found do:\n",
    "for filename in unique_filenames_list:\n",
    "    # Recreate the full filenames with extensions, and store each one of them into a single variable\n",
    "    json_file = filename + \".info.json\"\n",
    "    srv3_file = filename + \".\" + \"en.srv3\"\n",
    "    # Create the output filename using the input filename\n",
    "    output_xml = srv3_file + \".xml\"\n",
    "    output_txt = srv3_file + \".txt\"\n",
    "    # Create the XML element <text>, root element of the final output\n",
    "    text_tag = etree.Element(\"text\")\n",
    "\n",
    "    print(f\"Processing {json_file}\")\n",
    "    # Open the metadata JSON file:\n",
    "    metadata_file = json.loads(open(json_file, encoding=\"utf8\").read())\n",
    "    # Add a set of metadata points as attributes of the <text> element tag\n",
    "    text_tag.attrib[\"title\"] = str(metadata_file[\"fulltitle\"])\n",
    "    # Read the `upload_date` datapoint in the format YYYYMMDD, and split the three values\n",
    "    # for the day, the month, and the year; then assign them to three separate attributes\n",
    "    date = datetime.strptime(metadata_file[\"upload_date\"], \"%Y%m%d\")\n",
    "    text_tag.attrib[\"date_d\"] = str(date.day)\n",
    "    text_tag.attrib[\"date_m\"] = str(date.month)\n",
    "    text_tag.attrib[\"date_y\"] = str(date.year)\n",
    "    # Check if the 'like_count' metadata point is present, if not assign the value \"na\" to the 'like_count' attribute\n",
    "    text_tag.attrib[\"likes\"] = str(\n",
    "        metadata_file[\"like_count\"] if \"like_count\" in metadata_file else \"na\"\n",
    "    )\n",
    "\n",
    "    # Assign the attribute 'format' with a value of 'srv' to the <text> element tag\n",
    "    text_tag.attrib[\"format\"] = \"srv3\"\n",
    "\n",
    "    print(f\"Processing {srv3_file}\")\n",
    "    # Open the SRV3 file\n",
    "    f = open(srv3_file, \"r\", encoding=\"utf8\")\n",
    "    # Parse its XML contents using BeautifulSoup\n",
    "    soup = BeautifulSoup(f, features=\"xml\")\n",
    "    # If the attribute 'ac' (= autocaption) with value '255' is found in the <s> element tag then the subtitles are the result of autocaptioning;\n",
    "    # hence assign the value 'true' to the variable 'is_ac'. Otherwise assign the value 'false' to 'is_ac'\n",
    "    if soup.body.find(\"s\", attrs={\"ac\": True}):\n",
    "        is_ac = \"true\"\n",
    "    else:\n",
    "        is_ac = \"false\"\n",
    "\n",
    "    # Assign the value of 'is_ac' to the <text> element tag attribute 'autocaption'\n",
    "    text_tag.attrib[\"autocaption\"] = is_ac\n",
    "\n",
    "    # Create an empty list to store all the subtitles sentences to be later saved to a \"plain-text\" file\n",
    "    plain_text = []\n",
    "\n",
    "    # For each paragraph (i.e. each line of the subtitles) in the file do:\n",
    "    for sent in soup.body.find_all(\"p\"):\n",
    "        # Check if the textual content of the paragraph is longer than 1 character; this avoids adding empty paragraphs to the final output\n",
    "        if len(sent.get_text()) > 1:\n",
    "            # Create a <p> element tag inside the XML output\n",
    "            p_tag = etree.SubElement(text_tag, \"p\")\n",
    "            # Add the attribute 'time' (indicating the starting time of the paragraph) and assign it the value appearing in 't'\n",
    "            p_tag.attrib[\"time\"] = str(sent[\"t\"])\n",
    "            # Add the attribute 'is_ac' and assign it the value of the previously created variable 'is_ac'\n",
    "            p_tag.attrib[\"is_ac\"] = is_ac\n",
    "            p_tag.text = sent.get_text()\n",
    "            plain_text.append(sent.get_text())\n",
    "        # If the paragraph does not contain any text (i.e. its length is < 1), skip it\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    # Write the extracted data formatted in XML to the final XML structure\n",
    "    tree = etree.ElementTree(text_tag)\n",
    "    # Write the XML to the output file\n",
    "    tree.write(\n",
    "        output_xml, pretty_print=True, xml_declaration=True, encoding=\"utf-8\"\n",
    "    )\n",
    "    \n",
    "    # Write the subtitles sentences only to a plain text file, one sentence per line\n",
    "    with open(output_txt, \"w\", encoding=\"utf8\") as o:\n",
    "    # Merge together all the sentences collected in the 'plain_text' list using a new line (\"\\n\")\n",
    "        txt_contents = \"\\n\".join(plain_text)\n",
    "    # Write the merged sentences to the final .txt file\n",
    "        o.write(txt_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41e993b-e33a-46f6-a6ca-0d7b0095f0c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
